# **ğŸ¤– AI Tinkerers Hackathon - Supa Team WeRecooked ğŸ¤–**

<!-- Badge to Visit Project -->
<div align="center"> 
    <a href="https://your-streamlit-app-url.com">
        <img src="https://img.shields.io/badge/Visit%20AI%20Tinkerers%20Hackathon%20Project-brightgreen?style=for-the-badge&logo=streamlit" alt="Visit AI Tinkerers Hackathon Project"/>
    </a>
</div>

---

## **ğŸ“‹ Overview**

The **AI Tinkerers Hackathon Project - Supa Team WeRecooked** is an initiative aimed at building and benchmarking AI models, particularly focusing on developing Large Language Model (LLM) Judges. The project covers dataset preparation, benchmarking, finetuning models, and creating a user-friendly interface to showcase our results using Streamlit.

---

## **Table of Contents**

1. [ğŸ¯ Objectives](#-objectives)
2. [ğŸ”§ Technologies Used](#-technologies-used)
3. [ğŸ—‚ï¸ Directory Structure](#-directory-structure)
4. [ğŸ“ Key Components](#-key-components)
5. [ğŸ“Š Visual Elements and Data](#-visual-elements-and-data)
6. [ğŸ”„ Project Workflow](#-project-workflow)
7. [ğŸ‰ Conclusion](#-conclusion)
8. [ğŸ”® Future Enhancements](#-future-enhancements)
9. [ğŸ“š References](#-references)
10. [ğŸ“œ License](#-license)

---

## **ğŸ¯ Objectives**

- **ğŸ“Š Develop benchmark models**: Create and evaluate LLMs like OpenAI Mini 4.0 and Mistral for specific use cases.
- **ğŸ“ Dataset preparation**: Preprocess datasets for benchmarking AI models, including language translations and task-specific data.
- **ğŸ’» Finetune models**: Explore and fine-tune models to improve accuracy on human preference datasets.
- **ğŸš€ Showcase results**: Deploy results using Streamlit to create an interactive platform.

---

## **ğŸ”§ Technologies Used**

![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)

---

## **ğŸ—‚ï¸ Directory Structure**

The project structure is as follows:

```plaintext
.
â”œâ”€â”€ README.md
â”œâ”€â”€ datasets
â”‚   â”œâ”€â”€ boolq-english-train.jsonl
â”‚   â”œâ”€â”€ fib-malay-openai.jsonl
â”‚   â””â”€â”€ for_presentation/
â”œâ”€â”€ miscellaneous
â”‚   â””â”€â”€ AIT_Problemstatement2_SUPA.pdf
â”œâ”€â”€ notebooks-benchmarking-exercises
â”‚   â”œâ”€â”€ 03_benchmark_openaimini4_0_llmasajudge_v1_v2.ipynb
â”‚   â””â”€â”€ 03_benchmark_malaysian_mistral_llmasajudge_v2.ipynb
â”œâ”€â”€ notebooks-data-preparation
â”‚   â”œâ”€â”€ 01_dataset_prep_boolq_openai.ipynb
â”‚   â””â”€â”€ archive_01_dataset_prep_fib_t5.ipynb
â”œâ”€â”€ notebooks-finetuning-models
â”‚   â”œâ”€â”€ 02_finetune_v1_malaysian_debertav2_base.ipynb
â”‚   â””â”€â”€ 02_finetune_v2_malaysian_mistral_7b_32k_instructions_v4.ipynb
â””â”€â”€ requirements.txt
```

---

## **ğŸ“ Key Components**

- **ğŸ” Data Preparation**: Includes notebooks and scripts for preparing datasets such as BoolQ and FIB for different models (e.g., OpenAI and T5).
- **ğŸ“Š Benchmarking**: Focuses on evaluating the performance of various AI models such as OpenAI Mini 4.0 and Mistral LLM as Judges.
- **ğŸ”§ Model Finetuning**: Contains code to finetune models like Malaysian DeBERTaV2 and Mistral LLM.

---

## **ğŸ“Š Visual Elements and Data**

- **ğŸ“ Datasets**: Processed and raw datasets are available for English and Malay human preferences.
- **ğŸ“‰ Benchmarking Notebooks**: Jupyter notebooks that contain the evaluation of models using different datasets.
- **ğŸ–¥ï¸ Interactive Interface**: Results will be visualized and shared using Streamlit.

---

## **ğŸ”„ Project Workflow**

1. **ğŸ“‚ Environment Setup**:
   - Set up a virtual environment and install required dependencies using `requirements.txt`.

2. **ğŸ”¨ Data Processing**:
   - Prepare datasets for specific tasks, including translation and task-specific formatting.

3. **ğŸš€ Model Finetuning**:
   - Fine-tune models on preprocessed datasets and evaluate performance.

4. **ğŸ“Š Benchmarking**:
   - Benchmark models using evaluation notebooks to assess effectiveness in judging tasks.

5. **ğŸŒ Deployment**:
   - Deploy results to an interactive Streamlit app for presentation.

---

## **ğŸ‰ Conclusion**

This project showcases the effectiveness of AI models like OpenAI Mini 4.0 and Mistral in the task of LLM Judges, providing a platform for performance evaluation and model comparison. By fine-tuning models on human-preference datasets, we aim to develop more accurate AI models.

---

## **ğŸ”® Future Enhancements**

- **ğŸ“ˆ Advanced Benchmarking**: Expand model benchmarking to include more datasets and AI models.
- **ğŸ¤– Further Finetuning**: Apply more advanced techniques for finetuning models to improve performance.
- **ğŸŒ Enhanced UI**: Improve the Streamlit UI for better interaction and visualization.

---

## **ğŸ“š References**

- [OpenAI API Documentation](https://beta.openai.com/docs/)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)

---

## **ğŸ“œ License**

**Supa Team WeRecooked License**

All rights reserved. Unauthorized use or reproduction of any part of this project is prohibited. For usage and license inquiries, contact the project maintainers.

